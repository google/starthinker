{
  "license": "Licensed under the Apache License, Version 2.0",
  "copyright": "Copyright 2020 Google LLC",
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CM360 Data Transfer To Bigquery",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397f08b1-001",
        "colab_type": "text"
      },
      "source": [
        "#1. Install Dependencies\n",
        "First install the libraries needed to execute recipes, this only needs to be done once, then click play.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397f08b1-002",
        "colab_type": "code"
      },
      "source": [
        "!pip install git+https://github.com/google/starthinker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397f08b1-003",
        "colab_type": "text"
      },
      "source": [
        "#2. Get Cloud Project ID\n",
        "To run this recipe [requires a Google Cloud Project](https://github.com/google/starthinker/blob/master/tutorials/cloud_project.md), this only needs to be done once, then click play.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397f08b1-004",
        "colab_type": "code"
      },
      "source": [
        "CLOUD_PROJECT = 'PASTE PROJECT ID HERE'\n",
        "\n",
        "print(\"Cloud Project Set To: %s\" % CLOUD_PROJECT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397f08b1-005",
        "colab_type": "text"
      },
      "source": [
        "#3. Get Client Credentials\n",
        "To read and write to various endpoints requires [downloading client credentials](https://github.com/google/starthinker/blob/master/tutorials/cloud_client_installed.md), this only needs to be done once, then click play.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397f08b1-006",
        "colab_type": "code"
      },
      "source": [
        "CLIENT_CREDENTIALS = 'PASTE CLIENT CREDENTIALS HERE'\n",
        "\n",
        "print(\"Client Credentials Set To: %s\" % CLIENT_CREDENTIALS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397f08b1-007",
        "colab_type": "text"
      },
      "source": [
        "#4. Enter CM360 Data Transfer To Bigquery Parameters\n",
        "Move data from a DT bucket into a BigQuery table.\n",
        " 1. Ensure your user has <a href='https://developers.google.com/doubleclick-advertisers/dtv2/getting-started' target='_blank'>access to the bucket</a>.\n",
        " 1. Provide the DT bucket name to read from.\n",
        " 1. Provide the path of the files to read.\n",
        " 1. Each file is synchronized to a unique table.  Use a view or aggregate select.\n",
        "Modify the values below for your use case, can be done multiple times, then click play.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397f08b1-008",
        "colab_type": "code"
      },
      "source": [
        "FIELDS = {\n",
        "  'auth_read': 'user',  # Credentials used for reading data.\n",
        "  'auth_write': 'service',  # Credentials used for writing data.\n",
        "  'bucket': '',  # Name of bucket where DT files are stored.\n",
        "  'paths': [],  # List of prefixes to pull specific DT files.\n",
        "  'days': 2,  # Number of days back to synchronize.\n",
        "  'hours': 0,  # Number of hours back to synchronize.\n",
        "  'dataset': '',  # Existing dataset in BigQuery.\n",
        "}\n",
        "\n",
        "print(\"Parameters Set To: %s\" % FIELDS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397f08b1-009",
        "colab_type": "text"
      },
      "source": [
        "#5. Execute CM360 Data Transfer To Bigquery\n",
        "This does NOT need to be modified unless you are changing the recipe, click play.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397f08b1-010",
        "colab_type": "code"
      },
      "source": [
        "from starthinker.util.configuration import Configuration\n",
        "from starthinker.util.configuration import execute\n",
        "from starthinker.util.recipe import json_set_fields\n",
        "\n",
        "USER_CREDENTIALS = '/content/user.json'\n",
        "\n",
        "TASKS = [\n",
        "  {\n",
        "    'dt': {\n",
        "      'auth': 'user',\n",
        "      'from': {\n",
        "        'bucket': {'field': {'name': 'bucket','kind': 'string','order': 2,'default': '','description': 'Name of bucket where DT files are stored.'}},\n",
        "        'paths': {'field': {'name': 'paths','kind': 'string_list','order': 3,'default': [],'description': 'List of prefixes to pull specific DT files.'}},\n",
        "        'days': {'field': {'name': 'days','kind': 'integer','order': 4,'default': 2,'description': 'Number of days back to synchronize.'}},\n",
        "        'hours': {'field': {'name': 'hours','kind': 'integer','order': 5,'default': 0,'description': 'Number of hours back to synchronize.'}}\n",
        "      },\n",
        "      'to': {\n",
        "        'auth': 'user',\n",
        "        'dataset': {'field': {'name': 'dataset','kind': 'string','order': 6,'default': '','description': 'Existing dataset in BigQuery.'}}\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "json_set_fields(TASKS, FIELDS)\n",
        "\n",
        "execute(Configuration(project=CLOUD_PROJECT, client=CLIENT_CREDENTIALS, user=USER_CREDENTIALS, verbose=True), TASKS, force=True)\n"
      ]
    }
  ]
}
